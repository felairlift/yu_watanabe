{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import DataParallel\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetTemperature, nvmlShutdown, NVML_TEMPERATURE_GPU\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pynvml\n",
    "import optuna\n",
    "import random\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "\n",
    "def timecount():\n",
    "    end_time=time.time()\n",
    "    elapsed_time=end_time-start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)  # 3600秒 = 1時間\n",
    "    minutes, seconds = divmod(rem, 60)  # 60秒 = 1分\n",
    "    # 経過時間の表示\n",
    "    print(f\"処理にかかった時間: {int(hours)}時間 {int(minutes)}分 {seconds:.2f}秒\")\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"\n",
    "    Set seeds for all the major random number generators to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Python標準の乱数\n",
    "    np.random.seed(seed)  # NumPyの乱数\n",
    "    torch.manual_seed(seed)  # PyTorchのCPU乱数\n",
    "    torch.cuda.manual_seed_all(seed)  # PyTorchのGPU乱数\n",
    "\n",
    "    # 再現性のための設定\n",
    "    torch.backends.cudnn.deterministic = True  # 決定的な演算を有効化\n",
    "    torch.backends.cudnn.benchmark = False  # ベンチマークを無効化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.tensor(np.ones(shape, dtype='float32')))\n",
    "        self.beta = nn.Parameter(torch.tensor(np.zeros(shape, dtype='float32')))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, (0, 2, 3), keepdim=True)  # WRITE ME\n",
    "        std = torch.std(x, (0, 2, 3), keepdim=True)  # WRITE ME\n",
    "        x_normalized = (x - mean) / (std**2 + self.epsilon)**0.5  # WRITE ME\n",
    "        return self.gamma * x_normalized + self.beta  # WRITE ME\n",
    "    \n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 学習時はdropout_ratio分だけ出力をシャットアウト\n",
    "        if self.training:\n",
    "            self.mask = torch.rand(*x.size()) > self.dropout_ratio\n",
    "            return x * self.mask.to(x.device)\n",
    "        # 推論時は出力に`1.0 - self.dropout_ratio`を乗算することで学習時の出力の大きさに合わせる\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, filter_shape, function=lambda x: x, stride=(1, 1), padding=0):\n",
    "        super().__init__()\n",
    "        # Heの初期化\n",
    "        # filter_shape: (出力チャンネル数)x(入力チャンネル数)x(縦の次元数)x(横の次元数)\n",
    "        fan_in = filter_shape[1] * filter_shape[2] * filter_shape[3]\n",
    "        fan_out = filter_shape[0] * filter_shape[2] * filter_shape[3]\n",
    "\n",
    "        self.W = nn.Parameter(torch.tensor(rng.normal(\n",
    "                        0,\n",
    "                        np.sqrt(2/fan_in),\n",
    "                        size=filter_shape\n",
    "                    ).astype('float32')))\n",
    "\n",
    "        # バイアスはフィルタごとなので, 出力フィルタ数と同じ次元数\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros((filter_shape[0]), dtype='float32')))\n",
    "\n",
    "        self.function = function  # 活性化関数\n",
    "        self.stride = stride  # ストライド幅\n",
    "        self.padding = padding  # パディング\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n",
    "        return self.function(u)\n",
    "    \n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, ksize=(1, 1), stride=(1, 1), padding=0):\n",
    "        super().__init__()\n",
    "        self.ksize = ksize  # カーネルサイズ\n",
    "        self.stride = stride  # ストライド幅\n",
    "        self.padding = padding  # パディング\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.max_pool2d(x, kernel_size=self.ksize, stride=self.stride, padding=self.padding)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "    \n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        # Heの初期化\n",
    "        # in_dim: 入力の次元数，out_dim: 出力の次元数       \n",
    "        self.W = nn.Parameter(torch.tensor(rng.normal(\n",
    "                        0,\n",
    "                        np.sqrt(2/in_dim),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32')))\n",
    "        \n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([out_dim]).astype('float32')))\n",
    "        self.function = function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.function(torch.matmul(x, self.W) + self.b)\n",
    "    \n",
    "class Activation(nn.Module):\n",
    "\n",
    "    def __init__(self, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "\n",
    "def torch_log(x):\n",
    "    return torch.log(torch.clamp(x, min=1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prese_1pulse_dataloader_save.pyにて作成したデータを読み込む。\n",
    "# 保存したテンソルを読み込み\n",
    "X_train, y_train = torch.load('/mnt/sdb/ywatanabe/CNN_dataset/transwave/1pulse/dataset_train_200260.pt')\n",
    "X_valid, y_valid = torch.load('/mnt/sdb/ywatanabe/CNN_dataset/transwave/1pulse/dataset_valid_200260.pt')\n",
    "\n",
    "\n",
    "# PyTorchのテンソルをNumPy配列に変換\n",
    "X_train_np, y_train_np = X_train.numpy(), y_train.numpy()\n",
    "X_valid_np, y_valid_np = X_valid.numpy(), y_valid.numpy()\n",
    "\n",
    "# train_test_splitでデータを分割\n",
    "X_train_np, X_no_use_np, y_train_np, y_no_use_np = train_test_split(X_train_np, y_train_np, test_size=0.8, random_state=42)\n",
    "X_valid_np, X_no_use_np, y_valid_np, y_no_use_np = train_test_split(X_valid_np, y_valid_np, test_size=0.8, random_state=42)\n",
    "\n",
    "# NumPy配列を再びPyTorchテンソルに戻す\n",
    "X_train, y_train = torch.tensor(X_train_np), torch.tensor(y_train_np)\n",
    "X_valid, y_valid = torch.tensor(X_valid_np), torch.tensor(y_valid_np)\n",
    "\n",
    "\n",
    "# TensorDataset と DataLoader を再作成\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_valid = TensorDataset(X_valid, y_valid)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "        print(trial.number)\n",
    "        set_all_seeds(trial.number)\n",
    "                \n",
    "        device = torch.device(\"cuda:1\")  # メインのGPUを指定    optimizer = optim.SGD(conv_net.parameters(), lr=lr)\n",
    "        avg_train_mse=0.0\n",
    "        avg_valid_mse=0.0              \n",
    "        avg_train_mse_list=[]\n",
    "        avg_valid_mse_list=[]\n",
    "\n",
    "        # 相関係数の計算\n",
    "        correlation_coefficient_list=[]\n",
    "        mae_list = []\n",
    "        relative_error_list = []    \n",
    "        try:\n",
    "\n",
    "                #畳み込み層の数\n",
    "                filter_number = trial.suggest_int(\"filter_number\", 64, 128, step=32)\n",
    "                y_length = 15\n",
    "                x_length = 3112\n",
    "                x_conv = int(trial.suggest_int(\"x_conv\", 3, 5))\n",
    "                x_stride = int(trial.suggest_int(\"x_stride\", 1, 2))\n",
    "                conv_padding = 0\n",
    "                y_conv=1\n",
    "                y_stride=1\n",
    "                y_pool=1\n",
    "                x_pool = int(trial.suggest_int(\"x_pool\", 4, 8))\n",
    "                lr = trial.suggest_float('learning rate', 0.00001, 0.0005, log=True)\n",
    "                n_epochs = 20\n",
    "                Dense2_length = int(trial.suggest_int(\"mid_units\", 64, 128, step=32))\n",
    "                batch_size = int(trial.suggest_int(\"batch_size\", 32, 128, step=32))\n",
    "\n",
    "                loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "                loader_valid = DataLoader(dataset=dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "                # DataLoaderのサイズ（バッチ数）\n",
    "                num_batches = len(loader_train)\n",
    "                print(f'Number of batches in dataloader_train: {num_batches}')\n",
    "\n",
    "                # データセット全体のサイズ（サンプル数）\n",
    "                dataset_size = len(dataset_train)\n",
    "                print(f'Number of samples in dataset_train: {dataset_size}')\n",
    "                #del X_train,X_valid,y_train,y_valid,dataset_train,dataset_valid,X_train_np, X_no_use_np, y_train_np, y_no_use_np,X_valid_np, y_valid_np,\n",
    "            \n",
    "                \n",
    "                \n",
    "                path_in='/mnt/sdb/ywatanabe/Saved_png_after1130/transwave/20percent_input_200260/1pulse'\n",
    "                \n",
    "                path_in=os.path.join(path_in)\n",
    "                print(f\"Training with parameters: x_length={x_length}, x_conv={x_conv}, x_stride={x_stride}, x_pool={x_pool}, \"\n",
    "                        f\"filter_number={filter_number}, lr={lr}, n_epochs={n_epochs}, batch_size={batch_size}\")\n",
    "                Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"lr\"+str(lr)+\"batchsize\"+str(batch_size)\n",
    "                path_image=os.path.join(\"/mnt/sdb/ywatanabe/Saved_png_after1130/transwave/20percent_input/1pulse/mse\",Title+\".png\")\n",
    "                \n",
    "                B1_x_length=int((x_length+2*conv_padding-x_conv)//x_stride+1)\n",
    "                B2_x_length=int((B1_x_length//x_pool+2*conv_padding-x_conv)//x_stride+1)\n",
    "                B1_y_length=int((y_length+2*conv_padding-y_conv)//y_stride+1)\n",
    "                B2_y_length=int((B1_y_length//y_pool+2*conv_padding-y_conv)//y_stride+1)   \n",
    "                Dense_length=int((B2_x_length//x_pool)*(B2_y_length//y_pool)*filter_number)\n",
    "                conv_net = nn.Sequential(\n",
    "                Conv(filter_shape=(filter_number, 1, y_conv, x_conv), stride=(1, x_stride), padding=conv_padding),        # 画像の大きさ：1x4012x1 -> 1x3996x64  # WRITE ME(入出力の画像サイズ）\n",
    "                BatchNorm((filter_number, B1_y_length, B1_x_length)),    #15pulse分あるため\n",
    "                Activation(F.relu),\n",
    "                Pooling(ksize=(1, x_pool), stride=(1, x_pool), padding=conv_padding),            # 1x3996x64 -> 1x999x64  # WRITE ME(入出力の画像サイズ）\n",
    "                Conv(filter_shape=(filter_number, filter_number, y_conv, x_conv),stride=(1, x_stride)),       # 1x999x64 -> 1x983x64  # WRITE ME(入出力の画像サイズ）\n",
    "                BatchNorm((filter_number, B2_y_length, B2_x_length)),\n",
    "                Activation(F.relu),\n",
    "                Pooling(ksize=(1, x_pool), stride=(1, x_pool)),            # 1x983x64 -> 1x250x64????なぜかこうなってる  # WRITE ME(入出力の画像サイズ）\n",
    "                Flatten(),\n",
    "                Dense(Dense_length, Dense2_length, F.relu),  # 1\n",
    "                Dense(Dense2_length, 1)\n",
    "                )\n",
    "                \n",
    "                conv_net = conv_net.to(device)  # モデルを指定したGPUに移動\n",
    "                #conv_net = DataParallel(conv_net, device_ids=[ 1, 0, 2], output_device=1)  # GPU1を主デバイスに設定\n",
    "                criterion = nn.MSELoss()\n",
    "                #optimizer = get_optimizer(trial, conv_net)\n",
    "                optimizer = optim.SGD(conv_net.parameters(), lr=lr)\n",
    "\n",
    "                \n",
    "                for epoch in range(n_epochs):\n",
    "                        #train\n",
    "                        #memoricount()    \n",
    "                        conv_net.train()  # 訓練モードにする]\n",
    "                        total_trainloss = 0.0\n",
    "                        for x, t in loader_train:\n",
    "                                conv_net.zero_grad()  # 勾配の初期化\n",
    "                                x = x.to(device).float()  # テンソルをGPUに移動し、データ型をfloat32に変換\n",
    "                                t = t.to(device).float()\n",
    "                                y = conv_net.forward(x)  # 順伝播\n",
    "                                y = y.squeeze(dim=1)\n",
    "                                loss = criterion(y, t)\n",
    "                                loss.backward()  # 誤差の逆伝播\n",
    "                                optimizer.step()  # パラメータの更新\n",
    "                                total_trainloss += loss.item()\n",
    "                        avg_train_mse = total_trainloss / len(loader_train)\n",
    "                        avg_train_mse_list.append(avg_train_mse)\n",
    "\n",
    "                        #test\n",
    "                        y_list = []\n",
    "                        t_list = []\n",
    "                        total_validloss = 0.0\n",
    "\n",
    "                        conv_net.eval()\n",
    "                        with torch.no_grad():\n",
    "                                for x, t in loader_valid:\n",
    "                                        x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "                                        t = t.to(device)\n",
    "                                        y = conv_net.forward(x)  # 順伝播\n",
    "                                        y = y.squeeze(dim=1)\n",
    "\n",
    "                                        loss = criterion(y, t)\n",
    "                                        y_list.extend(y)\n",
    "                                        t_list.extend(t)\n",
    "                                                \n",
    "                                        # バッチごとの損失を蓄積\n",
    "                                        total_validloss += loss.item()\n",
    "                        avg_valid_mse = total_validloss / len(loader_valid)\n",
    "                        avg_valid_mse_list.append(avg_valid_mse)\n",
    "                        if np.isnan(avg_train_mse):\n",
    "                                print(\"Loss became NaN. Stopping training.\")\n",
    "                                \n",
    "                                break\n",
    "                        if np.isnan(avg_valid_mse):\n",
    "                                print(\"Loss became NaN. Stopping training.\")\n",
    "                                break\n",
    "                        if epoch%5==4:\n",
    "                                timecount()\n",
    "                                print('EPOCH: {}, Train [Loss: {:.6f} ], Valid [Loss: {:.6f} ]'.format(\n",
    "                                        epoch,\n",
    "                                        avg_train_mse,\n",
    "                                        avg_valid_mse\n",
    "                                        ))\n",
    "\n",
    "                        # y_list と t_list をそれぞれ numpy 配列に変換\n",
    "                        y_array = np.array([y.cpu().numpy() for y in y_list])\n",
    "                        t_array = np.array([t.cpu().numpy() for t in t_list])\n",
    "\n",
    "                        # グラフを描画\n",
    "                        plt.figure(figsize=(10, 10))\n",
    "                        plt.scatter(t_array, y_array, alpha=0.08)  # 散布図として描画\n",
    "                        #plt.plot(t_array, y_array, linestyle='-', alpha=0.2)  # 折れ線としても表示することで変化を見やすく\n",
    "                        plt.plot([0, 0.2], [0, 0.2], color='red', linestyle='--', label='y = x')  # 基準線\n",
    "                        plt.xlim(0,0.2)\n",
    "                        plt.ylim(0,0.2)\n",
    "                        plt.xlabel(\"t_list (Ground Truth)\")\n",
    "                        plt.ylabel(\"y_list (Predicted)\")\n",
    "                        Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"epoch\"+str(epoch)+\"lr\"+str(lr)\n",
    "                        plt.title(Title)\n",
    "                        plt.grid(True)\n",
    "                        path_image=os.path.join(path_in,\"plot\",Title+\".png\")\n",
    "                        plt.savefig(path_image, format=\"png\", dpi=300)\n",
    "                        #plt.show()\n",
    "                        plt.close()\n",
    "                        # グラフを描画\n",
    "\n",
    "                        plt.figure(figsize=(12, 10))\n",
    "                                # ヒストグラムを2Dカラーマップで描画\n",
    "                        plt.hist2d(t_array.flatten(), y_array.flatten(), bins=40, range=[[0, 0.2], [0, 0.2]], cmap='viridis', cmin=0, density=True)\n",
    "                        plt.colorbar(label=\"Frequency\")  # カラーバーを追加    \n",
    "                        plt.plot([0, 0.2], [0, 0.2], color='red', linestyle='--', label='y = x')  # 基準線\n",
    "                        plt.xlim(0,0.2)\n",
    "                        plt.ylim(0,0.2)\n",
    "                        plt.xlabel(\"t_list (Ground Truth)\")\n",
    "                        plt.ylabel(\"y_list (Predicted)\")\n",
    "                        Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"epoch\"+str(epoch)+\"lr\"+str(lr)+\"midlength\"+str(Dense2_length)\n",
    "                        plt.title(Title)\n",
    "                        plt.grid(True)\n",
    "                        path_image=os.path.join(path_in,\"colormap\",Title+\".png\")\n",
    "                        plt.savefig(path_image, format=\"png\", dpi=300)\n",
    "                        #plt.show()\n",
    "                        plt.close()\n",
    "\n",
    "                        correlation_coefficient = np.corrcoef(y_array.flatten(), t_array.flatten())[0, 1]# 相関係数の計算\n",
    "                        mae = np.mean(np.abs(y_array - t_array))# MAE (Mean Absolute Error) の計算\n",
    "                        relative_error = np.mean(np.abs(y_array - t_array) / (np.abs(t_array) + 1e-8))  # 相対誤差 (Relative Error) の計算# 真の値が0の場合はゼロ除算を防ぐために条件を追加\n",
    "                        correlation_coefficient_list.append(correlation_coefficient)\n",
    "                        mae_list.append(mae)\n",
    "                        relative_error_list.append(relative_error)\n",
    "                if not np.isnan(avg_valid_mse):\n",
    "\n",
    "                        plt.figure(figsize=(10, 10))\n",
    "                        plt.plot(range(1,n_epochs), avg_train_mse_list[1:])\n",
    "                        plt.plot(range(1,n_epochs), avg_valid_mse_list[1:], c='#00ff00')\n",
    "                        #print(avg_valid_mse_list)\n",
    "                        #plt.xlim(0, n_epochs)\n",
    "                        #plt.ylim(0, 2.5)\n",
    "                        plt.xticks(np.arange(0, 20, 5))\n",
    "\n",
    "                        plt.xlabel('EPOCH')\n",
    "                        plt.ylabel('LOSS')\n",
    "                        plt.legend(['train loss', 'test loss'])\n",
    "                        Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"lr\"+str(lr)+\"batchsize\"+str(batch_size)+\"mid_length\"+str(Dense2_length)\n",
    "                        plt.title(Title)\n",
    "                        path_image=os.path.join(path_in,\"mse\",Title+\".png\")\n",
    "                        plt.savefig(path_image, format=\"png\", dpi=300)\n",
    "                        plt.close()\n",
    "\n",
    "                        # 保存するデータ\n",
    "                        data = [\n",
    "                                {\"epoch\": epoch + 1,\n",
    "                                \"correlation_coefficient\": correlation_coefficient_list[epoch],\n",
    "                                \"mae\": mae_list[epoch],\n",
    "                                \"train_mse\": avg_train_mse_list[epoch],\n",
    "                                \"valid_mse\": avg_valid_mse_list[epoch],\n",
    "                                \"relative_error\": relative_error_list[epoch]}\n",
    "                                for epoch in range(n_epochs)\n",
    "                        ]\n",
    "                        # 保存するCSVファイル名\n",
    "                        path_csv=os.path.join(path_in,\"csv\",Title+\".csv\")\n",
    "                        # CSVファイルに書き込み\n",
    "                        with open(path_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                                # ヘッダーを作成\n",
    "                                fieldnames = [\"epoch\", \"correlation_coefficient\", \"mae\", \"train_mse\",\"valid_mse\",\"relative_error\"]\n",
    "                                writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                                # ヘッダーを書き込み\n",
    "                                writer.writeheader()\n",
    "                                # データを書き込み\n",
    "                                writer.writerows(data)\n",
    "\n",
    "                        #wait_for_gpu_temperature(threshold=65,  check_interval=3*60)\n",
    "                        print(datetime.datetime.now())\n",
    "                        time.sleep(1*60)\n",
    "        except RuntimeWarning as e:\n",
    "                print(f\"RuntimeWarning: {e}\")\n",
    "                avg_valid_mse = np.nan\n",
    "        except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                avg_valid_mse = np.nan\n",
    "\n",
    "\n",
    "        return avg_valid_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in dataloader_train: 195\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=4, filter_number=96, lr=0.00025872714757899004, n_epochs=20, batch_size=64\n",
      "処理にかかった時間: 0時間 3分 5.33秒\n",
      "EPOCH: 4, Train [Loss: 0.001045 ], Valid [Loss: 0.001046 ]\n",
      "処理にかかった時間: 0時間 6分 8.51秒\n",
      "EPOCH: 9, Train [Loss: 0.000974 ], Valid [Loss: 0.000975 ]\n",
      "処理にかかった時間: 0時間 9分 11.07秒\n",
      "EPOCH: 14, Train [Loss: 0.000939 ], Valid [Loss: 0.000947 ]\n",
      "処理にかかった時間: 0時間 12分 13.77秒\n",
      "EPOCH: 19, Train [Loss: 0.000917 ], Valid [Loss: 0.000936 ]\n",
      "2024-12-03 14:28:41.961144\n",
      "Number of batches in dataloader_train: 195\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=7, filter_number=128, lr=0.00025219833211220875, n_epochs=20, batch_size=64\n",
      "処理にかかった時間: 0時間 16分 31.23秒\n",
      "EPOCH: 4, Train [Loss: 0.001287 ], Valid [Loss: 0.001290 ]\n",
      "処理にかかった時間: 0時間 19分 47.16秒\n",
      "EPOCH: 9, Train [Loss: 0.001145 ], Valid [Loss: 0.001164 ]\n",
      "処理にかかった時間: 0時間 23分 3.54秒\n",
      "EPOCH: 14, Train [Loss: 0.001095 ], Valid [Loss: 0.001112 ]\n",
      "処理にかかった時間: 0時間 26分 20.57秒\n",
      "EPOCH: 19, Train [Loss: 0.001073 ], Valid [Loss: 0.001093 ]\n",
      "2024-12-03 14:42:48.784410\n",
      "Number of batches in dataloader_train: 390\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=5, filter_number=96, lr=8.140081724557429e-05, n_epochs=20, batch_size=32\n",
      "処理にかかった時間: 0時間 30分 17.16秒\n",
      "EPOCH: 4, Train [Loss: 0.001438 ], Valid [Loss: 0.001460 ]\n",
      "処理にかかった時間: 0時間 33分 10.79秒\n",
      "EPOCH: 9, Train [Loss: 0.001123 ], Valid [Loss: 0.001145 ]\n",
      "処理にかかった時間: 0時間 36分 3.87秒\n",
      "EPOCH: 14, Train [Loss: 0.001000 ], Valid [Loss: 0.001028 ]\n",
      "処理にかかった時間: 0時間 38分 56.06秒\n",
      "EPOCH: 19, Train [Loss: 0.000972 ], Valid [Loss: 0.000998 ]\n",
      "2024-12-03 14:55:24.258014\n",
      "Number of batches in dataloader_train: 195\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=4, x_stride=2, x_pool=4, filter_number=64, lr=5.565152232693056e-05, n_epochs=20, batch_size=64\n",
      "処理にかかった時間: 0時間 40分 56.19秒\n",
      "EPOCH: 4, Train [Loss: 0.006584 ], Valid [Loss: 0.006037 ]\n",
      "処理にかかった時間: 0時間 41分 56.81秒\n",
      "EPOCH: 9, Train [Loss: 0.004779 ], Valid [Loss: 0.004619 ]\n",
      "処理にかかった時間: 0時間 42分 57.97秒\n",
      "EPOCH: 14, Train [Loss: 0.003876 ], Valid [Loss: 0.003821 ]\n",
      "処理にかかった時間: 0時間 43分 58.81秒\n",
      "EPOCH: 19, Train [Loss: 0.003478 ], Valid [Loss: 0.003587 ]\n",
      "2024-12-03 15:00:26.940418\n",
      "Number of batches in dataloader_train: 130\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=4, x_stride=1, x_pool=5, filter_number=64, lr=0.00022830991366917447, n_epochs=20, batch_size=96\n",
      "処理にかかった時間: 0時間 46分 52.78秒\n",
      "EPOCH: 4, Train [Loss: 0.004125 ], Valid [Loss: 0.003599 ]\n",
      "処理にかかった時間: 0時間 48分 48.16秒\n",
      "EPOCH: 9, Train [Loss: 0.001570 ], Valid [Loss: 0.001577 ]\n",
      "処理にかかった時間: 0時間 50分 43.44秒\n",
      "EPOCH: 14, Train [Loss: 0.001274 ], Valid [Loss: 0.001290 ]\n",
      "処理にかかった時間: 0時間 52分 38.49秒\n",
      "EPOCH: 19, Train [Loss: 0.001200 ], Valid [Loss: 0.001221 ]\n",
      "2024-12-03 15:09:06.663175\n",
      "Number of batches in dataloader_train: 390\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=4, filter_number=96, lr=0.00016571155425029987, n_epochs=20, batch_size=32\n",
      "処理にかかった時間: 0時間 56分 46.90秒\n",
      "EPOCH: 4, Train [Loss: 0.001116 ], Valid [Loss: 0.001095 ]\n",
      "処理にかかった時間: 0時間 59分 55.42秒\n",
      "EPOCH: 9, Train [Loss: 0.000991 ], Valid [Loss: 0.000985 ]\n",
      "処理にかかった時間: 1時間 3分 3.58秒\n",
      "EPOCH: 14, Train [Loss: 0.000950 ], Valid [Loss: 0.000949 ]\n",
      "処理にかかった時間: 1時間 6分 12.75秒\n",
      "EPOCH: 19, Train [Loss: 0.000924 ], Valid [Loss: 0.000927 ]\n",
      "2024-12-03 15:22:40.934304\n",
      "Number of batches in dataloader_train: 390\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=4, filter_number=96, lr=0.00019137384682446282, n_epochs=20, batch_size=32\n",
      "処理にかかった時間: 1時間 10分 21.57秒\n",
      "EPOCH: 4, Train [Loss: 0.001098 ], Valid [Loss: 0.001074 ]\n",
      "処理にかかった時間: 1時間 13分 30.24秒\n",
      "EPOCH: 9, Train [Loss: 0.001039 ], Valid [Loss: 0.001678 ]\n",
      "処理にかかった時間: 1時間 16分 38.61秒\n",
      "EPOCH: 14, Train [Loss: 0.000996 ], Valid [Loss: 0.000968 ]\n",
      "処理にかかった時間: 1時間 19分 47.19秒\n",
      "EPOCH: 19, Train [Loss: 0.000968 ], Valid [Loss: 0.001142 ]\n",
      "2024-12-03 15:36:15.386810\n",
      "Number of batches in dataloader_train: 390\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=4, filter_number=96, lr=0.00016618238232465307, n_epochs=20, batch_size=32\n",
      "処理にかかった時間: 1時間 23分 55.94秒\n",
      "EPOCH: 4, Train [Loss: 0.001044 ], Valid [Loss: 0.001197 ]\n",
      "処理にかかった時間: 1時間 27分 4.08秒\n",
      "EPOCH: 9, Train [Loss: 0.000983 ], Valid [Loss: 0.000974 ]\n",
      "処理にかかった時間: 1時間 30分 12.80秒\n",
      "EPOCH: 14, Train [Loss: 0.000952 ], Valid [Loss: 0.001086 ]\n",
      "処理にかかった時間: 1時間 33分 21.10秒\n",
      "EPOCH: 19, Train [Loss: 0.000933 ], Valid [Loss: 0.001003 ]\n",
      "2024-12-03 15:49:49.457141\n",
      "Number of batches in dataloader_train: 195\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=4, filter_number=96, lr=0.00025223902967175374, n_epochs=20, batch_size=64\n",
      "処理にかかった時間: 1時間 37分 25.45秒\n",
      "EPOCH: 4, Train [Loss: 0.001325 ], Valid [Loss: 0.001324 ]\n",
      "処理にかかった時間: 1時間 40分 28.87秒\n",
      "EPOCH: 9, Train [Loss: 0.001124 ], Valid [Loss: 0.001128 ]\n",
      "処理にかかった時間: 1時間 43分 31.68秒\n",
      "EPOCH: 14, Train [Loss: 0.000986 ], Valid [Loss: 0.001000 ]\n",
      "処理にかかった時間: 1時間 46分 34.39秒\n",
      "EPOCH: 19, Train [Loss: 0.000943 ], Valid [Loss: 0.000964 ]\n",
      "2024-12-03 16:03:02.611323\n",
      "Number of batches in dataloader_train: 195\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 5.46 GB\n",
      "Training with parameters: x_length=3112, x_conv=3, x_stride=1, x_pool=5, filter_number=96, lr=0.000242138395279059, n_epochs=20, batch_size=64\n",
      "処理にかかった時間: 1時間 50分 22.73秒\n",
      "EPOCH: 4, Train [Loss: 0.001517 ], Valid [Loss: 0.001502 ]\n",
      "処理にかかった時間: 1時間 53分 9.97秒\n",
      "EPOCH: 9, Train [Loss: 0.001227 ], Valid [Loss: 0.001232 ]\n",
      "処理にかかった時間: 1時間 55分 57.38秒\n",
      "EPOCH: 14, Train [Loss: 0.001121 ], Valid [Loss: 0.001143 ]\n",
      "処理にかかった時間: 1時間 58分 44.68秒\n",
      "EPOCH: 19, Train [Loss: 0.001085 ], Valid [Loss: 0.001112 ]\n",
      "2024-12-03 16:15:12.890357\n",
      "Number of batches in dataloader_train: 195\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 2.52 GB\n",
      "Training with parameters: x_length=3112, x_conv=4, x_stride=1, x_pool=4, filter_number=128, lr=0.00040510546706859344, n_epochs=20, batch_size=64\n",
      "処理にかかった時間: 2時間 3分 58.83秒\n",
      "EPOCH: 4, Train [Loss: 0.001373 ], Valid [Loss: 0.001192 ]\n",
      "処理にかかった時間: 2時間 8分 12.61秒\n",
      "EPOCH: 9, Train [Loss: 0.001302 ], Valid [Loss: 0.001512 ]\n",
      "処理にかかった時間: 2時間 12分 25.35秒\n",
      "EPOCH: 14, Train [Loss: 0.001258 ], Valid [Loss: 0.001503 ]\n",
      "処理にかかった時間: 2時間 16分 39.15秒\n",
      "EPOCH: 19, Train [Loss: 0.001256 ], Valid [Loss: 0.001495 ]\n",
      "2024-12-03 16:33:07.197914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 327 failed with parameters: {'filter_number': 128, 'x_conv': 4, 'x_stride': 1, 'x_pool': 4, 'learning rate': 0.00040510546706859344, 'mid_units': 96, 'batch_size': 64} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_753008/826425256.py\", line 223, in objective\n",
      "    time.sleep(1*60)\n",
      "KeyboardInterrupt\n",
      "Trial 327 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m TRIAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(storage\u001b[38;5;241m=\u001b[39mstorage_name, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1pulse_20percent_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRIAL_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# optuna-dashboard sqlite:////mnt/sdb/ywatanabe/CNN_dataset/transwave/optuna_data/1pulse_20percent_input.db --port 6002\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 上記分をターミナルで実行すればよい\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[17], line 223\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[38;5;66;03m#wait_for_gpu_temperature(threshold=65,  check_interval=3*60)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m--> 223\u001b[0m                 \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntimeWarning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "db_path = \"/mnt/sdb/ywatanabe/CNN_dataset/transwave/optuna_data/1pulse_20percent_input_200260.db\"\n",
    "storage_name = f\"sqlite:///{db_path}\"\n",
    "\n",
    "\n",
    "TRIAL_SIZE = 100\n",
    "study = optuna.create_study(storage=storage_name, study_name=\"1pulse_20percent_input\", load_if_exists=True)\n",
    "study.optimize(objective, n_trials=TRIAL_SIZE)\n",
    "# optuna-dashboard sqlite:////mnt/sdb/ywatanabe/CNN_dataset/transwave/optuna_data/1pulse_20percent_input.db --port 6002\n",
    "# 上記分をターミナルで実行すればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best value: {study.best_value}')\n",
    "print(f'Best param: {study.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "#db_path = \"/mnt/sdb/ywatanabe/CNN_dataset/optuna_data/example.db\"\n",
    "if False:\n",
    "\n",
    "    # データベースに接続\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # データベース内のテーブルを確認\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    print(cursor.fetchall())\n",
    "\n",
    "    # 特定のテーブル内容を確認（例: `trials` テーブル）\n",
    "    cursor.execute(\"SELECT * FROM trials;\")\n",
    "    print(cursor.fetchall())\n",
    "\n",
    "    # 接続を閉じる\n",
    "    conn.close()\n",
    "# optuna-dashboard sqlite:////mnt/sdb/ywatanabe/CNN_dataset/reflectedwave/optuna_data/20percent_input.db --port 8000\n",
    "# 上記分をターミナルで実行すればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
