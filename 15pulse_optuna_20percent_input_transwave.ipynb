{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import DataParallel\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetTemperature, nvmlShutdown, NVML_TEMPERATURE_GPU\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pynvml\n",
    "import optuna\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "start_time=time.time()\n",
    "\n",
    "def timecount():\n",
    "    end_time=time.time()\n",
    "    elapsed_time=end_time-start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)  # 3600秒 = 1時間\n",
    "    minutes, seconds = divmod(rem, 60)  # 60秒 = 1分\n",
    "    # 経過時間の表示\n",
    "    print(f\"処理にかかった時間: {int(hours)}時間 {int(minutes)}分 {seconds:.2f}秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.tensor(np.ones(shape, dtype='float32')))\n",
    "        self.beta = nn.Parameter(torch.tensor(np.zeros(shape, dtype='float32')))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, (0, 2, 3), keepdim=True)  # WRITE ME\n",
    "        std = torch.std(x, (0, 2, 3), keepdim=True)  # WRITE ME\n",
    "        x_normalized = (x - mean) / (std**2 + self.epsilon)**0.5  # WRITE ME\n",
    "        return self.gamma * x_normalized + self.beta  # WRITE ME\n",
    "    \n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 学習時はdropout_ratio分だけ出力をシャットアウト\n",
    "        if self.training:\n",
    "            self.mask = torch.rand(*x.size()) > self.dropout_ratio\n",
    "            return x * self.mask.to(x.device)\n",
    "        # 推論時は出力に`1.0 - self.dropout_ratio`を乗算することで学習時の出力の大きさに合わせる\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, filter_shape, function=lambda x: x, stride=(1, 1), padding=0):\n",
    "        super().__init__()\n",
    "        # Heの初期化\n",
    "        fan_in = filter_shape[1] * filter_shape[2] * filter_shape[3]\n",
    "        self.W = nn.Parameter(torch.randn(filter_shape) * (2 / fan_in) ** 0.5)\n",
    "        self.b = nn.Parameter(torch.zeros(filter_shape[0]))\n",
    "        self.function = function\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n",
    "        return self.function(u)\n",
    "\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, ksize=(1, 1), stride=(1, 1), padding=0):\n",
    "        super().__init__()\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.max_pool2d(x, kernel_size=self.ksize, stride=self.stride, padding=self.padding)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(in_dim, out_dim) * (2 / in_dim) ** 0.5)\n",
    "        self.b = nn.Parameter(torch.zeros(out_dim))\n",
    "        self.function = function\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.function(torch.matmul(x, self.W) + self.b)\n",
    "    \n",
    "class Activation(nn.Module):\n",
    "\n",
    "    def __init__(self, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "\n",
    "def torch_log(x):\n",
    "    return torch.log(torch.clamp(x, min=1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in dataloader_train: 390\n",
      "Number of samples in dataset_train: 12456\n",
      "  Used Memory: 0.24 GB\n"
     ]
    }
   ],
   "source": [
    "#prese_1pulse_dataloader_save.pyにて作成したデータを読み込む。\n",
    "# 保存したテンソルを読み込み\n",
    "X_train, y_train = torch.load('/mnt/sdb/ywatanabe/CNN_dataset/transwave/15pulse/dataset_train_1129.pt')\n",
    "X_valid, y_valid = torch.load('/mnt/sdb/ywatanabe/CNN_dataset/transwave/15pulse/dataset_valid_1129.pt')\n",
    "\n",
    "\n",
    "# PyTorchのテンソルをNumPy配列に変換\n",
    "X_train_np, y_train_np = X_train.numpy(), y_train.numpy()\n",
    "X_valid_np, y_valid_np = X_valid.numpy(), y_valid.numpy()\n",
    "\n",
    "# train_test_splitでデータを分割\n",
    "X_train_np, X_no_use_np, y_train_np, y_no_use_np = train_test_split(X_train_np, y_train_np, test_size=0.8, random_state=22)\n",
    "X_valid_np, X_no_use_np, y_valid_np, y_no_use_np = train_test_split(X_valid_np, y_valid_np, test_size=0.8, random_state=22)\n",
    "\n",
    "# NumPy配列を再びPyTorchテンソルに戻す\n",
    "X_train, y_train = torch.tensor(X_train_np), torch.tensor(y_train_np)\n",
    "X_valid, y_valid = torch.tensor(X_valid_np), torch.tensor(y_valid_np)\n",
    "\n",
    "\n",
    "# TensorDataset と DataLoader を再作成\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_valid = TensorDataset(X_valid, y_valid)\n",
    "batch_size = 64\n",
    "\n",
    "loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset=dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "# DataLoaderのサイズ（バッチ数）\n",
    "num_batches = len(loader_train)\n",
    "print(f'Number of batches in dataloader_train: {num_batches}')\n",
    "\n",
    "# データセット全体のサイズ（サンプル数）\n",
    "dataset_size = len(dataset_train)\n",
    "print(f'Number of samples in dataset_train: {dataset_size}')\n",
    "del X_train,X_valid,y_train,y_valid,dataset_train,dataset_valid,X_train_np, X_no_use_np, y_train_np, y_no_use_np,X_valid_np, y_valid_np,\n",
    "memoricount()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "                \n",
    "        device = torch.device(\"cuda:0\")  # メインのGPUを指定    optimizer = optim.SGD(conv_net.parameters(), lr=lr)\n",
    "        avg_train_mse=0.0\n",
    "        avg_valid_mse=0.0              \n",
    "        avg_train_mse_list=[]\n",
    "        avg_valid_mse_list=[]\n",
    "\n",
    "        # 相関係数の計算\n",
    "        correlation_coefficient_list=[]\n",
    "        mae_list = []\n",
    "        relative_error_list = []    \n",
    "        try:\n",
    "\n",
    "                #畳み込み層の数\n",
    "                filter_number = trial.suggest_int(\"filter_number\", 64, 128)\n",
    "                y_length = 15\n",
    "                x_length = 3112\n",
    "                x_conv = int(trial.suggest_int(\"x_conv\", 2, 60))\n",
    "                x_stride = int(trial.suggest_int(\"x_stride\", 1, 2))\n",
    "                conv_padding = 0\n",
    "                y_conv=2\n",
    "                y_stride=1\n",
    "                y_pool=1\n",
    "                x_pool = int(trial.suggest_int(\"x_pool\", 4, 8))\n",
    "                lr = trial.suggest_float('learning rate', 0.0001, 0.0005, log=True)\n",
    "                n_epochs = 20\n",
    "                Dense2_length = int(trial.suggest_int(\"mid_units\", 64, 128))\n",
    "            \n",
    "                \n",
    "                \n",
    "                path_in='/mnt/sdb/ywatanabe/Saved_png_after1130/transwave/20percent_input/15pulse'\n",
    "                \n",
    "                path_in=os.path.join(path_in,\"y_conv=\"+str(y_conv))\n",
    "                print(f\"Training with parameters: x_length={x_length}, x_conv={x_conv}, x_stride={x_stride}, x_pool={x_pool}, \"\n",
    "                        f\"filter_number={filter_number}, lr={lr}, n_epochs={n_epochs}\")\n",
    "                Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"lr\"+str(lr)+\"batchsize\"+str(batch_size)\n",
    "                path_image=os.path.join(\"/mnt/sdb/ywatanabe/Saved_png_after1130/transwave/20percent_input/15pulse/mse\",Title+\".png\")\n",
    "                \n",
    "                B1_x_length=int((x_length+2*conv_padding-x_conv)//x_stride+1)\n",
    "                B2_x_length=int((B1_x_length//x_pool+2*conv_padding-x_conv)//x_stride+1)\n",
    "                B1_y_length=int((y_length+2*conv_padding-y_conv)//y_stride+1)\n",
    "                B2_y_length=int((B1_y_length//y_pool+2*conv_padding-y_conv)//y_stride+1)   \n",
    "                Dense_length=int((B2_x_length//x_pool)*(B2_y_length//y_pool)*filter_number)\n",
    "                conv_net = nn.Sequential(\n",
    "                Conv(filter_shape=(filter_number, 1, y_conv, x_conv), stride=(1, x_stride), padding=conv_padding),        # 画像の大きさ：1x4012x1 -> 1x3996x64  # WRITE ME(入出力の画像サイズ）\n",
    "                BatchNorm((filter_number, B1_y_length, B1_x_length)),    #15pulse分あるため\n",
    "                Activation(F.relu),\n",
    "                Pooling(ksize=(1, x_pool), stride=(1, x_pool), padding=conv_padding),            # 1x3996x64 -> 1x999x64  # WRITE ME(入出力の画像サイズ）\n",
    "                Conv(filter_shape=(filter_number, filter_number, y_conv, x_conv),stride=(1, x_stride)),       # 1x999x64 -> 1x983x64  # WRITE ME(入出力の画像サイズ）\n",
    "                BatchNorm((filter_number, B2_y_length, B2_x_length)),\n",
    "                Activation(F.relu),\n",
    "                Pooling(ksize=(1, x_pool), stride=(1, x_pool)),            # 1x983x64 -> 1x250x64????なぜかこうなってる  # WRITE ME(入出力の画像サイズ）\n",
    "                Flatten(),\n",
    "                Dense(Dense_length, Dense2_length, F.relu),  # 1\n",
    "                Dense(Dense2_length, 1)\n",
    "                )\n",
    "                \n",
    "                conv_net = conv_net.to(device)  # モデルを指定したGPUに移動\n",
    "                #conv_net = DataParallel(conv_net, device_ids=[ 1, 0, 2], output_device=1)  # GPU1を主デバイスに設定\n",
    "                criterion = nn.MSELoss()\n",
    "                #optimizer = get_optimizer(trial, conv_net)\n",
    "                optimizer = optim.SGD(conv_net.parameters(), lr=lr)\n",
    "\n",
    "                \n",
    "                for epoch in range(n_epochs):\n",
    "                        #train\n",
    "                        #memoricount()    \n",
    "                        conv_net.train()  # 訓練モードにする]\n",
    "                        total_trainloss = 0.0\n",
    "                        for x, t in loader_train:\n",
    "                                conv_net.zero_grad()  # 勾配の初期化\n",
    "                                x = x.to(device).float()  # テンソルをGPUに移動し、データ型をfloat32に変換\n",
    "                                t = t.to(device).float()\n",
    "                                y = conv_net.forward(x)  # 順伝播\n",
    "                                y = y.squeeze(dim=1)\n",
    "                                loss = criterion(y, t)\n",
    "                                loss.backward()  # 誤差の逆伝播\n",
    "                                optimizer.step()  # パラメータの更新\n",
    "                                total_trainloss += loss.item()\n",
    "                        avg_train_mse = total_trainloss / len(loader_train)\n",
    "                        avg_train_mse_list.append(avg_train_mse)\n",
    "\n",
    "                        #test\n",
    "                        y_list = []\n",
    "                        t_list = []\n",
    "                        total_validloss = 0.0\n",
    "\n",
    "                        conv_net.eval()\n",
    "                        with torch.no_grad():\n",
    "                                for x, t in loader_valid:\n",
    "                                        x = x.to(device)  # テンソルをGPUに移動\n",
    "\n",
    "                                        t = t.to(device)\n",
    "                                        y = conv_net.forward(x)  # 順伝播\n",
    "                                        y = y.squeeze(dim=1)\n",
    "\n",
    "                                        loss = criterion(y, t)\n",
    "                                        y_list.extend(y)\n",
    "                                        t_list.extend(t)\n",
    "                                                \n",
    "                                        # バッチごとの損失を蓄積\n",
    "                                        total_validloss += loss.item()\n",
    "                        avg_valid_mse = total_validloss / len(loader_valid)\n",
    "                        avg_valid_mse_list.append(avg_valid_mse)\n",
    "                        if np.isnan(avg_train_mse):\n",
    "                                print(\"Loss became NaN. Stopping training.\")\n",
    "                                \n",
    "                                break\n",
    "                        if np.isnan(avg_valid_mse):\n",
    "                                print(\"Loss became NaN. Stopping training.\")\n",
    "                                break\n",
    "                        if epoch%5==4:\n",
    "                                timecount()\n",
    "                                print('EPOCH: {}, Train [Loss: {:.6f} ], Valid [Loss: {:.6f} ]'.format(\n",
    "                                        epoch,\n",
    "                                        avg_train_mse,\n",
    "                                        avg_valid_mse\n",
    "                                        ))\n",
    "\n",
    "                        # y_list と t_list をそれぞれ numpy 配列に変換\n",
    "                        y_array = np.array([y.cpu().numpy() for y in y_list])\n",
    "                        t_array = np.array([t.cpu().numpy() for t in t_list])\n",
    "\n",
    "                        # グラフを描画\n",
    "                        plt.figure(figsize=(10, 10))\n",
    "                        plt.scatter(t_array, y_array, alpha=0.08)  # 散布図として描画\n",
    "                        #plt.plot(t_array, y_array, linestyle='-', alpha=0.2)  # 折れ線としても表示することで変化を見やすく\n",
    "                        plt.plot([0, 0.2], [0, 0.2], color='red', linestyle='--', label='y = x')  # 基準線\n",
    "                        plt.xlim(0,0.2)\n",
    "                        plt.ylim(0,0.2)\n",
    "                        plt.xlabel(\"t_list (Ground Truth)\")\n",
    "                        plt.ylabel(\"y_list (Predicted)\")\n",
    "                        Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"epoch\"+str(epoch)+\"lr\"+str(lr)\n",
    "                        plt.title(Title)\n",
    "                        plt.grid(True)\n",
    "                        path_image=os.path.join(path_in,\"plot\",Title+\".png\")\n",
    "                        plt.savefig(path_image, format=\"png\", dpi=300)\n",
    "                        #plt.show()\n",
    "                        plt.close()\n",
    "                        # グラフを描画\n",
    "\n",
    "                        plt.figure(figsize=(12, 10))\n",
    "                                # ヒストグラムを2Dカラーマップで描画\n",
    "                        plt.hist2d(t_array.flatten(), y_array.flatten(), bins=40, range=[[0, 0.2], [0, 0.2]], cmap='viridis', cmin=0, density=True)\n",
    "                        plt.colorbar(label=\"Frequency\")  # カラーバーを追加    \n",
    "                        plt.plot([0, 0.2], [0, 0.2], color='red', linestyle='--', label='y = x')  # 基準線\n",
    "                        plt.xlim(0,0.2)\n",
    "                        plt.ylim(0,0.2)\n",
    "                        plt.xlabel(\"t_list (Ground Truth)\")\n",
    "                        plt.ylabel(\"y_list (Predicted)\")\n",
    "                        Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"epoch\"+str(epoch)+\"lr\"+str(lr)+\"midlength\"+str(Dense2_length)\n",
    "                        plt.title(Title)\n",
    "                        plt.grid(True)\n",
    "                        path_image=os.path.join(path_in,\"colormap\",Title+\".png\")\n",
    "                        plt.savefig(path_image, format=\"png\", dpi=300)\n",
    "                        #plt.show()\n",
    "                        plt.close()\n",
    "\n",
    "                        correlation_coefficient = np.corrcoef(y_array.flatten(), t_array.flatten())[0, 1]# 相関係数の計算\n",
    "                        mae = np.mean(np.abs(y_array - t_array))# MAE (Mean Absolute Error) の計算\n",
    "                        relative_error = np.mean(np.abs(y_array - t_array) / (np.abs(t_array) + 1e-8))  # 相対誤差 (Relative Error) の計算# 真の値が0の場合はゼロ除算を防ぐために条件を追加\n",
    "                        correlation_coefficient_list.append(correlation_coefficient)\n",
    "                        mae_list.append(mae)\n",
    "                        relative_error_list.append(relative_error)\n",
    "                if not np.isnan(avg_valid_mse):\n",
    "\n",
    "                        plt.figure(figsize=(10, 10))\n",
    "                        plt.plot(range(1,n_epochs), avg_train_mse_list[1:])\n",
    "                        plt.plot(range(1,n_epochs), avg_valid_mse_list[1:], c='#00ff00')\n",
    "                        #print(avg_valid_mse_list)\n",
    "                        #plt.xlim(0, n_epochs)\n",
    "                        #plt.ylim(0, 2.5)\n",
    "                        plt.xticks(np.arange(0, 20, 5))\n",
    "\n",
    "                        plt.xlabel('EPOCH')\n",
    "                        plt.ylabel('LOSS')\n",
    "                        plt.legend(['train loss', 'test loss'])\n",
    "                        Title=\"x_conv\"+str(x_conv)+\"x_stride\"+str(x_stride)+\"x_pool\"+str(x_pool)+\"Filter_number\"+str(filter_number)+\"lr\"+str(lr)+\"batchsize\"+str(batch_size)+\"mid_length\"+str(Dense2_length)\n",
    "                        plt.title(Title)\n",
    "                        path_image=os.path.join(path_in,\"mse\",Title+\".png\")\n",
    "                        plt.savefig(path_image, format=\"png\", dpi=300)\n",
    "                        plt.close()\n",
    "\n",
    "                        # 保存するデータ\n",
    "                        data = [\n",
    "                                {\"epoch\": epoch + 1,\n",
    "                                \"correlation_coefficient\": correlation_coefficient_list[epoch],\n",
    "                                \"mae\": mae_list[epoch],\n",
    "                                \"train_mse\": avg_train_mse_list[epoch],\n",
    "                                \"valid_mse\": avg_valid_mse_list[epoch],\n",
    "                                \"relative_error\": relative_error_list[epoch]}\n",
    "                                for epoch in range(n_epochs)\n",
    "                        ]\n",
    "                        # 保存するCSVファイル名\n",
    "                        path_csv=os.path.join(path_in,\"csv\",Title+\".csv\")\n",
    "                        # CSVファイルに書き込み\n",
    "                        with open(path_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                                # ヘッダーを作成\n",
    "                                fieldnames = [\"epoch\", \"correlation_coefficient\", \"mae\", \"train_mse\",\"valid_mse\",\"relative_error\"]\n",
    "                                writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                                # ヘッダーを書き込み\n",
    "                                writer.writeheader()\n",
    "                                # データを書き込み\n",
    "                                writer.writerows(data)\n",
    "\n",
    "                        #wait_for_gpu_temperature(threshold=65,  check_interval=3*60)\n",
    "                        print(datetime.datetime.now())\n",
    "                        time.sleep(1*60)\n",
    "        except RuntimeWarning as e:\n",
    "                print(f\"RuntimeWarning: {e}\")\n",
    "                avg_valid_mse = np.nan\n",
    "        except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                avg_valid_mse = np.nan\n",
    "\n",
    "\n",
    "        return avg_valid_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: x_length=3112, x_conv=44, x_stride=2, x_pool=5, filter_number=73, lr=0.0003570772123003681, n_epochs=20\n",
      "処理にかかった時間: 0時間 3分 46.82秒\n",
      "EPOCH: 4, Train [Loss: 0.004283 ], Valid [Loss: 0.002380 ]\n",
      "処理にかかった時間: 0時間 7分 14.52秒\n",
      "EPOCH: 9, Train [Loss: 0.002719 ], Valid [Loss: 0.002685 ]\n",
      "処理にかかった時間: 0時間 10分 44.92秒\n",
      "EPOCH: 14, Train [Loss: 0.002355 ], Valid [Loss: 0.001437 ]\n",
      "処理にかかった時間: 0時間 14分 16.24秒\n",
      "EPOCH: 19, Train [Loss: 0.001934 ], Valid [Loss: 0.000970 ]\n",
      "2024-11-30 14:21:02.773860\n",
      "Training with parameters: x_length=3112, x_conv=55, x_stride=1, x_pool=8, filter_number=88, lr=0.00021461282869657697, n_epochs=20\n",
      "処理にかかった時間: 0時間 25分 37.51秒\n",
      "EPOCH: 4, Train [Loss: 0.001505 ], Valid [Loss: 0.000928 ]\n",
      "処理にかかった時間: 0時間 35分 57.56秒\n",
      "EPOCH: 9, Train [Loss: 0.001536 ], Valid [Loss: 0.000691 ]\n",
      "処理にかかった時間: 0時間 46分 17.37秒\n",
      "EPOCH: 14, Train [Loss: 0.001324 ], Valid [Loss: 0.000850 ]\n",
      "処理にかかった時間: 0時間 56分 42.65秒\n",
      "EPOCH: 19, Train [Loss: 0.001695 ], Valid [Loss: 0.001238 ]\n",
      "2024-11-30 15:03:29.030554\n",
      "Training with parameters: x_length=3112, x_conv=26, x_stride=2, x_pool=4, filter_number=120, lr=0.00024908884425565097, n_epochs=20\n",
      "処理にかかった時間: 1時間 2分 51.42秒\n",
      "EPOCH: 4, Train [Loss: 0.001634 ], Valid [Loss: 0.002848 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "処理にかかった時間: 1時間 7分 58.75秒\n",
      "EPOCH: 9, Train [Loss: 0.001973 ], Valid [Loss: 0.001848 ]\n",
      "処理にかかった時間: 1時間 13分 5.84秒\n",
      "EPOCH: 14, Train [Loss: 0.001430 ], Valid [Loss: 0.001486 ]\n",
      "処理にかかった時間: 1時間 18分 13.08秒\n",
      "EPOCH: 19, Train [Loss: 0.001430 ], Valid [Loss: 0.001488 ]\n",
      "2024-11-30 15:24:59.265152\n",
      "Training with parameters: x_length=3112, x_conv=6, x_stride=2, x_pool=4, filter_number=65, lr=0.00026633157372520566, n_epochs=20\n",
      "処理にかかった時間: 1時間 21分 14.24秒\n",
      "EPOCH: 4, Train [Loss: 0.000690 ], Valid [Loss: 0.000709 ]\n",
      "処理にかかった時間: 1時間 23分 14.98秒\n",
      "EPOCH: 9, Train [Loss: 0.000616 ], Valid [Loss: 0.000732 ]\n",
      "処理にかかった時間: 1時間 25分 15.51秒\n",
      "EPOCH: 14, Train [Loss: 0.000505 ], Valid [Loss: 0.000689 ]\n",
      "処理にかかった時間: 1時間 27分 16.55秒\n",
      "EPOCH: 19, Train [Loss: 0.000325 ], Valid [Loss: 0.001002 ]\n",
      "2024-11-30 15:34:03.294125\n",
      "Training with parameters: x_length=3112, x_conv=13, x_stride=2, x_pool=6, filter_number=93, lr=0.00031078991416327975, n_epochs=20\n",
      "処理にかかった時間: 1時間 31分 5.92秒\n",
      "EPOCH: 4, Train [Loss: 0.000740 ], Valid [Loss: 0.000690 ]\n",
      "処理にかかった時間: 1時間 33分 54.73秒\n",
      "EPOCH: 9, Train [Loss: 0.000650 ], Valid [Loss: 0.000669 ]\n",
      "処理にかかった時間: 1時間 36分 47.67秒\n",
      "EPOCH: 14, Train [Loss: 0.000599 ], Valid [Loss: 0.000946 ]\n",
      "処理にかかった時間: 1時間 39分 48.56秒\n",
      "EPOCH: 19, Train [Loss: 0.000412 ], Valid [Loss: 0.000659 ]\n",
      "2024-11-30 15:46:35.657418\n",
      "Training with parameters: x_length=3112, x_conv=23, x_stride=2, x_pool=7, filter_number=120, lr=0.00039727310148776087, n_epochs=20\n",
      "処理にかかった時間: 1時間 44分 46.40秒\n",
      "EPOCH: 4, Train [Loss: 0.000752 ], Valid [Loss: 0.000752 ]\n",
      "処理にかかった時間: 1時間 48分 42.87秒\n",
      "EPOCH: 9, Train [Loss: 0.000724 ], Valid [Loss: 0.000674 ]\n",
      "処理にかかった時間: 1時間 52分 39.81秒\n",
      "EPOCH: 14, Train [Loss: 0.000671 ], Valid [Loss: 0.000756 ]\n",
      "処理にかかった時間: 1時間 56分 36.33秒\n",
      "EPOCH: 19, Train [Loss: 0.000650 ], Valid [Loss: 0.000669 ]\n",
      "2024-11-30 16:03:22.659707\n",
      "Training with parameters: x_length=3112, x_conv=10, x_stride=2, x_pool=6, filter_number=122, lr=0.00023185980144746246, n_epochs=20\n",
      "処理にかかった時間: 2時間 1分 9.20秒\n",
      "EPOCH: 4, Train [Loss: 0.001804 ], Valid [Loss: 0.001269 ]\n",
      "処理にかかった時間: 2時間 4分 41.08秒\n",
      "EPOCH: 9, Train [Loss: 0.001787 ], Valid [Loss: 0.005147 ]\n",
      "処理にかかった時間: 2時間 8分 13.52秒\n",
      "EPOCH: 14, Train [Loss: 0.002030 ], Valid [Loss: 0.003590 ]\n",
      "処理にかかった時間: 2時間 11分 45.47秒\n",
      "EPOCH: 19, Train [Loss: 0.001639 ], Valid [Loss: 0.001019 ]\n",
      "2024-11-30 16:18:31.810797\n",
      "Training with parameters: x_length=3112, x_conv=48, x_stride=2, x_pool=7, filter_number=125, lr=0.0004481896695116197, n_epochs=20\n",
      "処理にかかった時間: 2時間 17分 49.70秒\n",
      "EPOCH: 4, Train [Loss: 0.000952 ], Valid [Loss: 0.000916 ]\n",
      "処理にかかった時間: 2時間 22分 51.59秒\n",
      "EPOCH: 9, Train [Loss: 0.003553 ], Valid [Loss: 0.007829 ]\n",
      "処理にかかった時間: 2時間 27分 50.80秒\n",
      "EPOCH: 14, Train [Loss: 0.003797 ], Valid [Loss: 0.008570 ]\n",
      "処理にかかった時間: 2時間 32分 50.20秒\n",
      "EPOCH: 19, Train [Loss: 0.003628 ], Valid [Loss: 0.001720 ]\n",
      "2024-11-30 16:39:36.521784\n",
      "Training with parameters: x_length=3112, x_conv=20, x_stride=2, x_pool=7, filter_number=82, lr=0.0004960046335376482, n_epochs=20\n",
      "処理にかかった時間: 2時間 36分 27.08秒\n",
      "EPOCH: 4, Train [Loss: 0.001203 ], Valid [Loss: 0.001977 ]\n",
      "処理にかかった時間: 2時間 39分 3.46秒\n",
      "EPOCH: 9, Train [Loss: 0.001584 ], Valid [Loss: 0.000919 ]\n",
      "処理にかかった時間: 2時間 41分 39.83秒\n",
      "EPOCH: 14, Train [Loss: 0.001644 ], Valid [Loss: 0.000829 ]\n",
      "処理にかかった時間: 2時間 44分 16.02秒\n",
      "EPOCH: 19, Train [Loss: 0.001247 ], Valid [Loss: 0.000854 ]\n",
      "2024-11-30 16:51:03.030741\n",
      "Training with parameters: x_length=3112, x_conv=20, x_stride=1, x_pool=7, filter_number=86, lr=0.0001606118330105645, n_epochs=20\n",
      "処理にかかった時間: 2時間 51分 53.54秒\n",
      "EPOCH: 4, Train [Loss: 0.000936 ], Valid [Loss: 0.000985 ]\n",
      "処理にかかった時間: 2時間 58分 31.50秒\n",
      "EPOCH: 9, Train [Loss: 0.001083 ], Valid [Loss: 0.001937 ]\n",
      "処理にかかった時間: 3時間 5分 16.40秒\n",
      "EPOCH: 14, Train [Loss: 0.000989 ], Valid [Loss: 0.000745 ]\n",
      "処理にかかった時間: 3時間 11分 52.97秒\n",
      "EPOCH: 19, Train [Loss: 0.001052 ], Valid [Loss: 0.000830 ]\n",
      "2024-11-30 17:18:39.354494\n",
      "Training with parameters: x_length=3112, x_conv=33, x_stride=1, x_pool=5, filter_number=104, lr=0.000104640065284368, n_epochs=20\n",
      "処理にかかった時間: 3時間 25分 55.59秒\n",
      "EPOCH: 4, Train [Loss: 0.000693 ], Valid [Loss: 0.000690 ]\n",
      "処理にかかった時間: 3時間 38分 56.99秒\n",
      "EPOCH: 9, Train [Loss: 0.000653 ], Valid [Loss: 0.000678 ]\n",
      "処理にかかった時間: 3時間 52分 3.58秒\n",
      "EPOCH: 14, Train [Loss: 0.000626 ], Valid [Loss: 0.000667 ]\n",
      "処理にかかった時間: 4時間 5分 5.04秒\n",
      "EPOCH: 19, Train [Loss: 0.000604 ], Valid [Loss: 0.000616 ]\n",
      "2024-11-30 18:11:51.410486\n",
      "Training with parameters: x_length=3112, x_conv=38, x_stride=1, x_pool=5, filter_number=102, lr=0.0001055549745485017, n_epochs=20\n",
      "処理にかかった時間: 4時間 19分 56.38秒\n",
      "EPOCH: 4, Train [Loss: 0.003788 ], Valid [Loss: 0.002327 ]\n",
      "処理にかかった時間: 4時間 33分 48.65秒\n",
      "EPOCH: 9, Train [Loss: 0.007429 ], Valid [Loss: 0.004527 ]\n",
      "処理にかかった時間: 4時間 47分 38.61秒\n",
      "EPOCH: 14, Train [Loss: 0.011877 ], Valid [Loss: 0.020720 ]\n",
      "処理にかかった時間: 5時間 1分 41.74秒\n",
      "EPOCH: 19, Train [Loss: 0.010695 ], Valid [Loss: 0.004628 ]\n",
      "2024-11-30 19:08:28.372881\n",
      "Training with parameters: x_length=3112, x_conv=34, x_stride=1, x_pool=5, filter_number=104, lr=0.00010403890280662114, n_epochs=20\n",
      "処理にかかった時間: 5時間 17分 16.31秒\n",
      "EPOCH: 4, Train [Loss: 0.074705 ], Valid [Loss: 0.003840 ]\n",
      "処理にかかった時間: 5時間 31分 25.27秒\n",
      "EPOCH: 9, Train [Loss: 0.124858 ], Valid [Loss: 0.015777 ]\n",
      "処理にかかった時間: 5時間 46分 2.21秒\n",
      "EPOCH: 14, Train [Loss: 0.059587 ], Valid [Loss: 0.008040 ]\n",
      "処理にかかった時間: 6時間 1分 23.93秒\n",
      "EPOCH: 19, Train [Loss: 0.023272 ], Valid [Loss: 0.039638 ]\n",
      "2024-11-30 20:08:11.054567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 64 failed with parameters: {'filter_number': 104, 'x_conv': 34, 'x_stride': 1, 'x_pool': 5, 'learning rate': 0.00010403890280662114, 'mid_units': 65} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_84217/1943865403.py\", line 209, in objective\n",
      "    time.sleep(1*60)\n",
      "KeyboardInterrupt\n",
      "Trial 64 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m TRIAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(storage\u001b[38;5;241m=\u001b[39mstorage_name, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20percent_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRIAL_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# optuna-dashboard sqlite:////mnt/sdb/ywatanabe/CNN_dataset/transwave/optuna_data/20percent_input_after1130.db --port 8081\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 上記分をターミナルで実行すればよい\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/sdb/ywatanabe/yuwatanabe/yuvenv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 209\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[38;5;66;03m#wait_for_gpu_temperature(threshold=65,  check_interval=3*60)\u001b[39;00m\n\u001b[1;32m    208\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[0;32m--> 209\u001b[0m                 \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntimeWarning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "db_path = \"/mnt/sdb/ywatanabe/CNN_dataset/transwave/optuna_data/20percent_input_after1130.db\"\n",
    "storage_name = f\"sqlite:///{db_path}\"\n",
    "\n",
    "\n",
    "TRIAL_SIZE = 100\n",
    "study = optuna.create_study(storage=storage_name, study_name=\"20percent_input\", load_if_exists=True)\n",
    "study.optimize(objective, n_trials=TRIAL_SIZE)\n",
    "# optuna-dashboard sqlite:////mnt/sdb/ywatanabe/CNN_dataset/transwave/optuna_data/20percent_input_after1130.db --port 8081\n",
    "# 上記分をターミナルで実行すればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value: 0.0005403973108124254\n",
      "Best param: {'filter_number': 108, 'x_conv': 31, 'x_stride': 3, 'x_pool': 4, 'learning rate': 0.0026290940542728703, 'mid_units': 56}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best value: {study.best_value}')\n",
    "print(f'Best param: {study.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "#db_path = \"/mnt/sdb/ywatanabe/CNN_dataset/optuna_data/example.db\"\n",
    "if False:\n",
    "\n",
    "    # データベースに接続\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # データベース内のテーブルを確認\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    print(cursor.fetchall())\n",
    "\n",
    "    # 特定のテーブル内容を確認（例: `trials` テーブル）\n",
    "    cursor.execute(\"SELECT * FROM trials;\")\n",
    "    print(cursor.fetchall())\n",
    "\n",
    "    # 接続を閉じる\n",
    "    conn.close()\n",
    "# optuna-dashboard sqlite:////mnt/sdb/ywatanabe/CNN_dataset/reflectedwave/optuna_data/20percent_input.db --port 8000\n",
    "# 上記分をターミナルで実行すればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
